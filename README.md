<p align="center">
  <img src="./docs/bark_wallpaper.png" style="max-width: 100%; height: auto;"/>
</p>

# üê∂ bark-cpp-python üêç

[![MIT license](https://img.shields.io/badge/License-MIT-blue.svg)](https://lbesson.mit-license.org/)
![Python](https://img.shields.io/badge/python-3.10%2B-pink.svg)


Python bindings for [bark.cpp](https://github.com/PABannier/bark.cpp) using `ctypes`. Utilize the power of GGML with bark, one of the most popular TTS models, and its quantized versions through a friendly Python interface üî•üî•üî•.

## ‚öôÔ∏è Feature
Inpsired by [llama-cpp-python](https://github.com/abetlen/llama-cpp-python), this package provides:

* [x] Low-level access to C API via `ctypes` interface
* [x] High-level Python API for TTS

## üöÄ Demo
This demo is tested on `AMD Ryzen 5 5600H`, `Ubuntu 20.04`
```bash
$ python test.py /home/ductm/Work/bark.cpp/models/bark-small/ggml_weights_q4_1.bin -p "Hi, I am Bark. Nice to meet you" -t 8 --dest output.wav

                 ___       _      ___     __  ___
 /\__/\  woof   |    \    / \    |    \  |  |/  /
/      \  woof  |    /   /   \   |    /  |     /
\      /        |    \  /  _  \  |  _ \  |     \
 \____/         |____/ /__/ \__\ |_| |_\ |__|\__\
    

encodec_load_model_weights: in_channels = 1
encodec_load_model_weights: hidden_dim  = 128
encodec_load_model_weights: n_filters   = 32
encodec_load_model_weights: kernel_size = 7
encodec_load_model_weights: res_kernel  = 3
encodec_load_model_weights: n_bins      = 1024
encodec_load_model_weights: bandwidth   = 24
encodec_load_model_weights: sample_rate = 24000
encodec_load_model_weights: ftype       = 1
encodec_load_model_weights: qntvr       = 0
encodec_load_model_weights: ggml tensor size    = 320 bytes
encodec_load_model_weights: backend buffer size =  54.36 MB
encodec_load_model_weights: using CPU backend
encodec_load_model_weights: model size =    44.36 MB
encodec_load_model: n_q = 32

bark_tokenize_input: prompt: 'Hi, I am Bark. Nice to meet you'
bark_tokenize_input: number of tokens in prompt = 513, first 8 tokens: 30113 10165 10194 20440 30746 20222 10167 36966 



bark_print_statistics:   sample time =    49.21 ms / 455 tokens
bark_print_statistics:  predict time =  3471.03 ms / 7.63 ms per token
bark_print_statistics:    total time =  3542.42 ms



bark_print_statistics:   sample time =    21.86 ms / 1364 tokens
bark_print_statistics:  predict time = 33798.57 ms / 24.78 ms per token
bark_print_statistics:    total time = 33829.69 ms



bark_print_statistics:   sample time =    70.14 ms / 6144 tokens
bark_print_statistics:  predict time =  8684.00 ms / 1.41 ms per token
bark_print_statistics:    total time =  8783.56 ms

encodec_eval: compute buffer size: 230.30 MB

Evaluated time: 47.49s
```
[output.webm](https://github.com/user-attachments/assets/5e1ca97c-f81f-4bc2-8118-41b007e7c33e)

## üîß Installation
The current stable version of `bark.cpp` and `encodec.cpp` are using `STATIC` as the only build type for their libraries. This makes it impossible to use them as shared libraries. To work-around this, until the pull requests I've made for modification of `bark.cpp` and `encodec.cpp` are accepted, we need to modify their `CMakeLists.txt` files a bit.

1. Clone the repo and submodules
```bash
git clone --recursive https://github.com/tranminhduc4796/bark-cpp-python.git

cd bark-cpp-python
```
2. Modify the CMakeLists.txt of `bark.cpp` and `encodec.cpp` by replacing these lines in their CMakeLists.txt:
```cmake
# vendor/bark.cpp/CMakeLists.txt
add_library(${BARK_LIB} STATIC bark.cpp bark.h)
# vendor/bark.cpp/encodec.cpp/CMakeLists.txt
add_library(${ENCODEC_LIB} STATIC encodec.cpp encodec.h)
```
with
```cmake
# vendor/bark.cpp/CMakeLists.txt
option(BUILD_SHARED_LIBS OFF)
add_library(${BARK_LIB} bark.cpp bark.h)
# vendor/bark.cpp/encodec.cpp/CMakeLists.txt
option(BUILD_SHARED_LIBS OFF)
add_library(${ENCODEC_LIB} encodec.cpp encodec.h)
```
3. Build and install
```bash
mkdir build
cd build
cmake ..
sudo make install -j8
```
## High-level Python API
```python
args = parse_arguments()

bark = Bark(
        model_path=args.model_path,
        temp=args.temp,
        fine_temp=args.fine_temp,
        min_eos_p=args.min_eos_p,
        sliding_window_size=args.sliding_window_size,
        max_coarse_history=args.max_coarse_history,
        sample_rate=args.sample_rate,
        target_bandwidth=args.target_bandwidth,
        n_steps_text_encoder=args.n_steps_text_encoder,
        semantic_rate_hz=args.semantic_rate_hz,
        coarse_rate_hz=args.coarse_rate_hz,
        seed=args.seed
    )
audio_arr = bark.generate_audio(args.prompt, args.threads)

print("Evaluated time: {:.2f}s".format(bark.get_eval_time() / 1e6))
bark.write_wav(args.dest, audio_arr)
```

## Acknowledgments
* [Suno AI's bark](https://github.com/suno-ai/bark)
* [bark.cpp](https://github.com/PABannier/bark.cpp)
